{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "* gym - collection of environments for reinforcement learning algorithms.\n",
    "* numpy - package for scientific computing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment\n",
    "The agent needs to navigate across a frozen lake with the snow melting in a few parts, to retreive a frisbee.\n",
    "The surface is described using a grid like the following:\n",
    "* SFFF       (S: starting point, safe)\n",
    "* FHFH       (F: frozen surface, safe)\n",
    "* FFFH       (H: hole, fall to your doom)\n",
    "* HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "* Every environment comes with an action_space and an observation_space. \n",
    "* These attributes are of type Space, and they describe the format of valid actions and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "* Define the learning rate (alpha) and the discount factor (gamma)\n",
    "* Create a Q-Table and intiialize it with 1 for each state-action pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = dict([(x, [1, 1, 1, 1]) for x in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define choose action method\n",
    "* Pick the action with the highest q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(observ):\n",
    "    return np.argmax(q_table[observ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "* Train the agent for 10000 episodes.\n",
    "* Invoke the reset method to start an episode.\n",
    "* Choose an arbitarary initial action.\n",
    "* Initialize previous observation and action\n",
    "* Iterate over 2500 timsteps. For each timestep: \n",
    "    * Render the environment for visulaization.\n",
    "    * Execute the chosen action and store the return values.\n",
    "    * Choose the next action based on observations returned by the previous action.\n",
    "    * If this is not the first action in the episode,\n",
    "        * get the q value for the previous action and previous observation.\n",
    "        * if s is not terminal,compute  Q(St, At)← Q(St, At) + α[Rt+1 + γQ(St+1, At+1) − Q(St, At)]. \n",
    "        * If the episode is over, diregard the discount factor when computing Q value.\n",
    "        * update the q table\n",
    "    * Update the previous observation and previous action parameters\n",
    "    * If the episode has terminated, print out the info. \n",
    "* Note that the executed action is not always the chosen action. Some episodes terminate in goal and some in a hole.         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    observ = env.reset()\n",
    "    \n",
    "    action = choose_action(observ)\n",
    "\n",
    "    prev_observ = None\n",
    "    prev_action = None\n",
    "\n",
    "    t = 0\n",
    "    \n",
    "    for t in range(2500):\n",
    "         \n",
    "        env.render()\n",
    "\n",
    "        observ, reward, done, info = env.step(action)\n",
    "\n",
    "        action = choose_action(observ)\n",
    "\n",
    "        if not prev_observ is None:\n",
    "\n",
    "            q_old = q_table[prev_observ][prev_action]\n",
    "            q_new = q_old\n",
    "\n",
    "            if done:\n",
    "                q_new += alpha * (reward - q_old)\n",
    "                \n",
    "            else:\n",
    "                q_new +=  alpha * (reward + gamma * q_table[observ][action] - q_old)\n",
    "\n",
    "            new_table = q_table[prev_observ]\n",
    "            new_table[prev_action] = q_new\n",
    "\n",
    "            q_table[prev_observ] = new_table\n",
    "\n",
    "        prev_observ = observ\n",
    "        prev_action = action\n",
    " \n",
    "            print(\"Episode {} finished after {} timesteps with r={}.\".format(i, t, reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
